{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251d46e29a8b42c3a75f9f133b76ab0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf464677b7b44188ea06e89917927c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1c49a611bf4059a2331541fda79b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cifar10/plain_text to /home/arefhz75/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0840cd6235ce4806a201bc77a22f1b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/170M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cifar10 downloaded and prepared to /home/arefhz75/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "#ds = load_dataset(\"scene_parse_150\", split=\"train[:30]\")\n",
    "ds = load_dataset(\"cifar10\", split=\"train[:30]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.3)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from huggingface_hub import cached_download, hf_hub_url\n",
    "\n",
    "repo_id = \"huggingface/label-files\"\n",
    "filename = \"ade20k-hf-doc-builder.json\"\n",
    "#id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type=\"dataset\")), \"r\"))\n",
    "id2label = {k: str(k) for k in range(180)}\n",
    "label2id = {str(k): k for k in range(180)}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /home/arefhz75/.cache/huggingface/hub/models--nvidia--mit-b0/snapshots/ed0b85c75627eab6a3c6989627450cf95f115381/preprocessor_config.json\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "{param_name} should be a dictionary on of the following set of keys: {VALID_SIZE_DICT_KEYS}, got {size}. Converted to {size_dict}.\n",
      "Image processor SegformerImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_reduce_labels\": false,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": false,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"SegformerImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 512,\n",
      "    \"width\": 512\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "feature_extractor = AutoImageProcessor.from_pretrained(\"nvidia/mit-b0\", reduce_labels=True, do_resize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ColorJitter\n",
    "\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_one(x):\n",
    "    ret = np.zeros((1,10))\n",
    "    ret[0,x] = 1\n",
    "    return ret\n",
    "def train_transforms(example_batch):\n",
    "    #images = [jitter(x) for x in example_batch[\"image\"]]\n",
    "    #print(\"batch   \", example_batch)\n",
    "    images = [x for x in example_batch[\"img\"]]\n",
    "    #print(\"im  \",images)\n",
    "    #labels = [np.array(hot_one(x)) for x in example_batch[\"label\"]]\n",
    "    labels = np.array([x for x in example_batch[\"label\"]])\n",
    "    #labels = [x for x in example_batch[\"annotation\"]]\n",
    "    #print(\"lb  \",labels)\n",
    "    inputs = feature_extractor(images, labels)\n",
    "    #inputs = feature_extractor(images)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "#     images = [x for x in example_batch[\"image\"]]\n",
    "#     labels = [x for x in example_batch[\"annotation\"]]\n",
    "    images = [x for x in example_batch[\"img\"]]\n",
    "    labels = np.array([x for x in example_batch[\"label\"]])\n",
    "    inputs = feature_extractor(images, labels)\n",
    "    return inputs\n",
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        logits_tensor = torch.nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        for key, value in metrics.items():\n",
    "            if type(value) is np.ndarray:\n",
    "                metrics[key] = value.tolist()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/arefhz75/.cache/huggingface/hub/models--nvidia--mit-b0/snapshots/ed0b85c75627eab6a3c6989627450cf95f115381/config.json\n",
      "Model config SegformerConfig {\n",
      "  \"_name_or_path\": \"nvidia/mit-b0\",\n",
      "  \"architectures\": [\n",
      "    \"SegformerForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"decoder_hidden_size\": 256,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"downsampling_rates\": [\n",
      "    1,\n",
      "    4,\n",
      "    8,\n",
      "    16\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_sizes\": [\n",
      "    32,\n",
      "    64,\n",
      "    160,\n",
      "    256\n",
      "  ],\n",
      "  \"id2label\": {\n",
      "    \"0\": \"0\",\n",
      "    \"1\": \"1\",\n",
      "    \"2\": \"2\",\n",
      "    \"3\": \"3\",\n",
      "    \"4\": \"4\",\n",
      "    \"5\": \"5\",\n",
      "    \"6\": \"6\",\n",
      "    \"7\": \"7\",\n",
      "    \"8\": \"8\",\n",
      "    \"9\": \"9\",\n",
      "    \"10\": \"10\",\n",
      "    \"11\": \"11\",\n",
      "    \"12\": \"12\",\n",
      "    \"13\": \"13\",\n",
      "    \"14\": \"14\",\n",
      "    \"15\": \"15\",\n",
      "    \"16\": \"16\",\n",
      "    \"17\": \"17\",\n",
      "    \"18\": \"18\",\n",
      "    \"19\": \"19\",\n",
      "    \"20\": \"20\",\n",
      "    \"21\": \"21\",\n",
      "    \"22\": \"22\",\n",
      "    \"23\": \"23\",\n",
      "    \"24\": \"24\",\n",
      "    \"25\": \"25\",\n",
      "    \"26\": \"26\",\n",
      "    \"27\": \"27\",\n",
      "    \"28\": \"28\",\n",
      "    \"29\": \"29\",\n",
      "    \"30\": \"30\",\n",
      "    \"31\": \"31\",\n",
      "    \"32\": \"32\",\n",
      "    \"33\": \"33\",\n",
      "    \"34\": \"34\",\n",
      "    \"35\": \"35\",\n",
      "    \"36\": \"36\",\n",
      "    \"37\": \"37\",\n",
      "    \"38\": \"38\",\n",
      "    \"39\": \"39\",\n",
      "    \"40\": \"40\",\n",
      "    \"41\": \"41\",\n",
      "    \"42\": \"42\",\n",
      "    \"43\": \"43\",\n",
      "    \"44\": \"44\",\n",
      "    \"45\": \"45\",\n",
      "    \"46\": \"46\",\n",
      "    \"47\": \"47\",\n",
      "    \"48\": \"48\",\n",
      "    \"49\": \"49\",\n",
      "    \"50\": \"50\",\n",
      "    \"51\": \"51\",\n",
      "    \"52\": \"52\",\n",
      "    \"53\": \"53\",\n",
      "    \"54\": \"54\",\n",
      "    \"55\": \"55\",\n",
      "    \"56\": \"56\",\n",
      "    \"57\": \"57\",\n",
      "    \"58\": \"58\",\n",
      "    \"59\": \"59\",\n",
      "    \"60\": \"60\",\n",
      "    \"61\": \"61\",\n",
      "    \"62\": \"62\",\n",
      "    \"63\": \"63\",\n",
      "    \"64\": \"64\",\n",
      "    \"65\": \"65\",\n",
      "    \"66\": \"66\",\n",
      "    \"67\": \"67\",\n",
      "    \"68\": \"68\",\n",
      "    \"69\": \"69\",\n",
      "    \"70\": \"70\",\n",
      "    \"71\": \"71\",\n",
      "    \"72\": \"72\",\n",
      "    \"73\": \"73\",\n",
      "    \"74\": \"74\",\n",
      "    \"75\": \"75\",\n",
      "    \"76\": \"76\",\n",
      "    \"77\": \"77\",\n",
      "    \"78\": \"78\",\n",
      "    \"79\": \"79\",\n",
      "    \"80\": \"80\",\n",
      "    \"81\": \"81\",\n",
      "    \"82\": \"82\",\n",
      "    \"83\": \"83\",\n",
      "    \"84\": \"84\",\n",
      "    \"85\": \"85\",\n",
      "    \"86\": \"86\",\n",
      "    \"87\": \"87\",\n",
      "    \"88\": \"88\",\n",
      "    \"89\": \"89\",\n",
      "    \"90\": \"90\",\n",
      "    \"91\": \"91\",\n",
      "    \"92\": \"92\",\n",
      "    \"93\": \"93\",\n",
      "    \"94\": \"94\",\n",
      "    \"95\": \"95\",\n",
      "    \"96\": \"96\",\n",
      "    \"97\": \"97\",\n",
      "    \"98\": \"98\",\n",
      "    \"99\": \"99\",\n",
      "    \"100\": \"100\",\n",
      "    \"101\": \"101\",\n",
      "    \"102\": \"102\",\n",
      "    \"103\": \"103\",\n",
      "    \"104\": \"104\",\n",
      "    \"105\": \"105\",\n",
      "    \"106\": \"106\",\n",
      "    \"107\": \"107\",\n",
      "    \"108\": \"108\",\n",
      "    \"109\": \"109\",\n",
      "    \"110\": \"110\",\n",
      "    \"111\": \"111\",\n",
      "    \"112\": \"112\",\n",
      "    \"113\": \"113\",\n",
      "    \"114\": \"114\",\n",
      "    \"115\": \"115\",\n",
      "    \"116\": \"116\",\n",
      "    \"117\": \"117\",\n",
      "    \"118\": \"118\",\n",
      "    \"119\": \"119\",\n",
      "    \"120\": \"120\",\n",
      "    \"121\": \"121\",\n",
      "    \"122\": \"122\",\n",
      "    \"123\": \"123\",\n",
      "    \"124\": \"124\",\n",
      "    \"125\": \"125\",\n",
      "    \"126\": \"126\",\n",
      "    \"127\": \"127\",\n",
      "    \"128\": \"128\",\n",
      "    \"129\": \"129\",\n",
      "    \"130\": \"130\",\n",
      "    \"131\": \"131\",\n",
      "    \"132\": \"132\",\n",
      "    \"133\": \"133\",\n",
      "    \"134\": \"134\",\n",
      "    \"135\": \"135\",\n",
      "    \"136\": \"136\",\n",
      "    \"137\": \"137\",\n",
      "    \"138\": \"138\",\n",
      "    \"139\": \"139\",\n",
      "    \"140\": \"140\",\n",
      "    \"141\": \"141\",\n",
      "    \"142\": \"142\",\n",
      "    \"143\": \"143\",\n",
      "    \"144\": \"144\",\n",
      "    \"145\": \"145\",\n",
      "    \"146\": \"146\",\n",
      "    \"147\": \"147\",\n",
      "    \"148\": \"148\",\n",
      "    \"149\": \"149\",\n",
      "    \"150\": \"150\",\n",
      "    \"151\": \"151\",\n",
      "    \"152\": \"152\",\n",
      "    \"153\": \"153\",\n",
      "    \"154\": \"154\",\n",
      "    \"155\": \"155\",\n",
      "    \"156\": \"156\",\n",
      "    \"157\": \"157\",\n",
      "    \"158\": \"158\",\n",
      "    \"159\": \"159\",\n",
      "    \"160\": \"160\",\n",
      "    \"161\": \"161\",\n",
      "    \"162\": \"162\",\n",
      "    \"163\": \"163\",\n",
      "    \"164\": \"164\",\n",
      "    \"165\": \"165\",\n",
      "    \"166\": \"166\",\n",
      "    \"167\": \"167\",\n",
      "    \"168\": \"168\",\n",
      "    \"169\": \"169\",\n",
      "    \"170\": \"170\",\n",
      "    \"171\": \"171\",\n",
      "    \"172\": \"172\",\n",
      "    \"173\": \"173\",\n",
      "    \"174\": \"174\",\n",
      "    \"175\": \"175\",\n",
      "    \"176\": \"176\",\n",
      "    \"177\": \"177\",\n",
      "    \"178\": \"178\",\n",
      "    \"179\": \"179\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"0\": 0,\n",
      "    \"1\": 1,\n",
      "    \"10\": 10,\n",
      "    \"100\": 100,\n",
      "    \"101\": 101,\n",
      "    \"102\": 102,\n",
      "    \"103\": 103,\n",
      "    \"104\": 104,\n",
      "    \"105\": 105,\n",
      "    \"106\": 106,\n",
      "    \"107\": 107,\n",
      "    \"108\": 108,\n",
      "    \"109\": 109,\n",
      "    \"11\": 11,\n",
      "    \"110\": 110,\n",
      "    \"111\": 111,\n",
      "    \"112\": 112,\n",
      "    \"113\": 113,\n",
      "    \"114\": 114,\n",
      "    \"115\": 115,\n",
      "    \"116\": 116,\n",
      "    \"117\": 117,\n",
      "    \"118\": 118,\n",
      "    \"119\": 119,\n",
      "    \"12\": 12,\n",
      "    \"120\": 120,\n",
      "    \"121\": 121,\n",
      "    \"122\": 122,\n",
      "    \"123\": 123,\n",
      "    \"124\": 124,\n",
      "    \"125\": 125,\n",
      "    \"126\": 126,\n",
      "    \"127\": 127,\n",
      "    \"128\": 128,\n",
      "    \"129\": 129,\n",
      "    \"13\": 13,\n",
      "    \"130\": 130,\n",
      "    \"131\": 131,\n",
      "    \"132\": 132,\n",
      "    \"133\": 133,\n",
      "    \"134\": 134,\n",
      "    \"135\": 135,\n",
      "    \"136\": 136,\n",
      "    \"137\": 137,\n",
      "    \"138\": 138,\n",
      "    \"139\": 139,\n",
      "    \"14\": 14,\n",
      "    \"140\": 140,\n",
      "    \"141\": 141,\n",
      "    \"142\": 142,\n",
      "    \"143\": 143,\n",
      "    \"144\": 144,\n",
      "    \"145\": 145,\n",
      "    \"146\": 146,\n",
      "    \"147\": 147,\n",
      "    \"148\": 148,\n",
      "    \"149\": 149,\n",
      "    \"15\": 15,\n",
      "    \"150\": 150,\n",
      "    \"151\": 151,\n",
      "    \"152\": 152,\n",
      "    \"153\": 153,\n",
      "    \"154\": 154,\n",
      "    \"155\": 155,\n",
      "    \"156\": 156,\n",
      "    \"157\": 157,\n",
      "    \"158\": 158,\n",
      "    \"159\": 159,\n",
      "    \"16\": 16,\n",
      "    \"160\": 160,\n",
      "    \"161\": 161,\n",
      "    \"162\": 162,\n",
      "    \"163\": 163,\n",
      "    \"164\": 164,\n",
      "    \"165\": 165,\n",
      "    \"166\": 166,\n",
      "    \"167\": 167,\n",
      "    \"168\": 168,\n",
      "    \"169\": 169,\n",
      "    \"17\": 17,\n",
      "    \"170\": 170,\n",
      "    \"171\": 171,\n",
      "    \"172\": 172,\n",
      "    \"173\": 173,\n",
      "    \"174\": 174,\n",
      "    \"175\": 175,\n",
      "    \"176\": 176,\n",
      "    \"177\": 177,\n",
      "    \"178\": 178,\n",
      "    \"179\": 179,\n",
      "    \"18\": 18,\n",
      "    \"19\": 19,\n",
      "    \"2\": 2,\n",
      "    \"20\": 20,\n",
      "    \"21\": 21,\n",
      "    \"22\": 22,\n",
      "    \"23\": 23,\n",
      "    \"24\": 24,\n",
      "    \"25\": 25,\n",
      "    \"26\": 26,\n",
      "    \"27\": 27,\n",
      "    \"28\": 28,\n",
      "    \"29\": 29,\n",
      "    \"3\": 3,\n",
      "    \"30\": 30,\n",
      "    \"31\": 31,\n",
      "    \"32\": 32,\n",
      "    \"33\": 33,\n",
      "    \"34\": 34,\n",
      "    \"35\": 35,\n",
      "    \"36\": 36,\n",
      "    \"37\": 37,\n",
      "    \"38\": 38,\n",
      "    \"39\": 39,\n",
      "    \"4\": 4,\n",
      "    \"40\": 40,\n",
      "    \"41\": 41,\n",
      "    \"42\": 42,\n",
      "    \"43\": 43,\n",
      "    \"44\": 44,\n",
      "    \"45\": 45,\n",
      "    \"46\": 46,\n",
      "    \"47\": 47,\n",
      "    \"48\": 48,\n",
      "    \"49\": 49,\n",
      "    \"5\": 5,\n",
      "    \"50\": 50,\n",
      "    \"51\": 51,\n",
      "    \"52\": 52,\n",
      "    \"53\": 53,\n",
      "    \"54\": 54,\n",
      "    \"55\": 55,\n",
      "    \"56\": 56,\n",
      "    \"57\": 57,\n",
      "    \"58\": 58,\n",
      "    \"59\": 59,\n",
      "    \"6\": 6,\n",
      "    \"60\": 60,\n",
      "    \"61\": 61,\n",
      "    \"62\": 62,\n",
      "    \"63\": 63,\n",
      "    \"64\": 64,\n",
      "    \"65\": 65,\n",
      "    \"66\": 66,\n",
      "    \"67\": 67,\n",
      "    \"68\": 68,\n",
      "    \"69\": 69,\n",
      "    \"7\": 7,\n",
      "    \"70\": 70,\n",
      "    \"71\": 71,\n",
      "    \"72\": 72,\n",
      "    \"73\": 73,\n",
      "    \"74\": 74,\n",
      "    \"75\": 75,\n",
      "    \"76\": 76,\n",
      "    \"77\": 77,\n",
      "    \"78\": 78,\n",
      "    \"79\": 79,\n",
      "    \"8\": 8,\n",
      "    \"80\": 80,\n",
      "    \"81\": 81,\n",
      "    \"82\": 82,\n",
      "    \"83\": 83,\n",
      "    \"84\": 84,\n",
      "    \"85\": 85,\n",
      "    \"86\": 86,\n",
      "    \"87\": 87,\n",
      "    \"88\": 88,\n",
      "    \"89\": 89,\n",
      "    \"9\": 9,\n",
      "    \"90\": 90,\n",
      "    \"91\": 91,\n",
      "    \"92\": 92,\n",
      "    \"93\": 93,\n",
      "    \"94\": 94,\n",
      "    \"95\": 95,\n",
      "    \"96\": 96,\n",
      "    \"97\": 97,\n",
      "    \"98\": 98,\n",
      "    \"99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-06,\n",
      "  \"mlp_ratios\": [\n",
      "    4,\n",
      "    4,\n",
      "    4,\n",
      "    4\n",
      "  ],\n",
      "  \"model_type\": \"segformer\",\n",
      "  \"num_attention_heads\": [\n",
      "    1,\n",
      "    2,\n",
      "    5,\n",
      "    8\n",
      "  ],\n",
      "  \"num_channels\": 3,\n",
      "  \"num_encoder_blocks\": 4,\n",
      "  \"patch_sizes\": [\n",
      "    7,\n",
      "    3,\n",
      "    3,\n",
      "    3\n",
      "  ],\n",
      "  \"reshape_last_stage\": true,\n",
      "  \"semantic_loss_ignore_index\": 255,\n",
      "  \"sr_ratios\": [\n",
      "    8,\n",
      "    4,\n",
      "    2,\n",
      "    1\n",
      "  ],\n",
      "  \"strides\": [\n",
      "    4,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\"\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/arefhz75/.cache/huggingface/hub/models--nvidia--mit-b0/snapshots/ed0b85c75627eab6a3c6989627450cf95f115381/pytorch_model.bin\n",
      "Some weights of the model checkpoint at nvidia/mit-b0 were not used when initializing SegformerForSemanticSegmentation: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.2.proj.bias', 'decode_head.classifier.bias', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.0.proj.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.0.proj.weight', 'decode_head.batch_norm.running_var', 'decode_head.linear_fuse.weight', 'decode_head.batch_norm.running_mean', 'decode_head.linear_c.2.proj.weight', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (nn_model): SegformerForSemanticSegmentation(\n",
      "    (segformer): SegformerModel(\n",
      "      (encoder): SegformerEncoder(\n",
      "        (patch_embeddings): ModuleList(\n",
      "          (0): SegformerOverlapPatchEmbeddings(\n",
      "            (proj): Conv2d(3, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "            (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (1): SegformerOverlapPatchEmbeddings(\n",
      "            (proj): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "            (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): SegformerOverlapPatchEmbeddings(\n",
      "            (proj): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "            (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (3): SegformerOverlapPatchEmbeddings(\n",
      "            (proj): Conv2d(160, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (block): ModuleList(\n",
      "          (0): ModuleList(\n",
      "            (0): SegformerLayer(\n",
      "              (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SegformerAttention(\n",
      "                (self): SegformerEfficientSelfAttention(\n",
      "                  (query): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (key): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (value): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n",
      "                  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                (output): SegformerSelfOutput(\n",
      "                  (dense): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): SegformerMixFFN(\n",
      "                (dense1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                (dwconv): SegformerDWConv(\n",
      "                  (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                )\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "                (dense2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SegformerLayer(\n",
      "              (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SegformerAttention(\n",
      "                (self): SegformerEfficientSelfAttention(\n",
      "                  (query): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (key): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (value): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n",
      "                  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                (output): SegformerSelfOutput(\n",
      "                  (dense): Linear(in_features=32, out_features=32, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SegformerDropPath(p=0.014285714365541935)\n",
      "              (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): SegformerMixFFN(\n",
      "                (dense1): Linear(in_features=32, out_features=128, bias=True)\n",
      "                (dwconv): SegformerDWConv(\n",
      "                  (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "                )\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "                (dense2): Linear(in_features=128, out_features=32, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): ModuleList(\n",
      "            (0): SegformerLayer(\n",
      "              (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SegformerAttention(\n",
      "                (self): SegformerEfficientSelfAttention(\n",
      "                  (query): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (key): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (value): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
      "                  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                (output): SegformerSelfOutput(\n",
      "                  (dense): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SegformerDropPath(p=0.02857142873108387)\n",
      "              (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): SegformerMixFFN(\n",
      "                (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                (dwconv): SegformerDWConv(\n",
      "                  (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "                )\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "                (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SegformerLayer(\n",
      "              (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SegformerAttention(\n",
      "                (self): SegformerEfficientSelfAttention(\n",
      "                  (query): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (key): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (value): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
      "                  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                (output): SegformerSelfOutput(\n",
      "                  (dense): Linear(in_features=64, out_features=64, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SegformerDropPath(p=0.04285714402794838)\n",
      "              (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): SegformerMixFFN(\n",
      "                (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
      "                (dwconv): SegformerDWConv(\n",
      "                  (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "                )\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "                (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): ModuleList(\n",
      "            (0): SegformerLayer(\n",
      "              (layer_norm_1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SegformerAttention(\n",
      "                (self): SegformerEfficientSelfAttention(\n",
      "                  (query): Linear(in_features=160, out_features=160, bias=True)\n",
      "                  (key): Linear(in_features=160, out_features=160, bias=True)\n",
      "                  (value): Linear(in_features=160, out_features=160, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n",
      "                  (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                (output): SegformerSelfOutput(\n",
      "                  (dense): Linear(in_features=160, out_features=160, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SegformerDropPath(p=0.05714285746216774)\n",
      "              (layer_norm_2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): SegformerMixFFN(\n",
      "                (dense1): Linear(in_features=160, out_features=640, bias=True)\n",
      "                (dwconv): SegformerDWConv(\n",
      "                  (dwconv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n",
      "                )\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "                (dense2): Linear(in_features=640, out_features=160, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SegformerLayer(\n",
      "              (layer_norm_1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SegformerAttention(\n",
      "                (self): SegformerEfficientSelfAttention(\n",
      "                  (query): Linear(in_features=160, out_features=160, bias=True)\n",
      "                  (key): Linear(in_features=160, out_features=160, bias=True)\n",
      "                  (value): Linear(in_features=160, out_features=160, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n",
      "                  (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "                (output): SegformerSelfOutput(\n",
      "                  (dense): Linear(in_features=160, out_features=160, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SegformerDropPath(p=0.0714285746216774)\n",
      "              (layer_norm_2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): SegformerMixFFN(\n",
      "                (dense1): Linear(in_features=160, out_features=640, bias=True)\n",
      "                (dwconv): SegformerDWConv(\n",
      "                  (dwconv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n",
      "                )\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "                (dense2): Linear(in_features=640, out_features=160, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (3): ModuleList(\n",
      "            (0): SegformerLayer(\n",
      "              (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SegformerAttention(\n",
      "                (self): SegformerEfficientSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SegformerSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SegformerDropPath(p=0.08571428805589676)\n",
      "              (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): SegformerMixFFN(\n",
      "                (dense1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                (dwconv): SegformerDWConv(\n",
      "                  (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "                )\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "                (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SegformerLayer(\n",
      "              (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SegformerAttention(\n",
      "                (self): SegformerEfficientSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SegformerSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SegformerDropPath(p=0.10000000149011612)\n",
      "              (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): SegformerMixFFN(\n",
      "                (dense1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                (dwconv): SegformerDWConv(\n",
      "                  (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
      "                )\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "                (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (layer_norm): ModuleList(\n",
      "          (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "          (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decode_head): SegformerDecodeHead(\n",
      "      (linear_c): ModuleList(\n",
      "        (0): SegformerMLP(\n",
      "          (proj): Linear(in_features=32, out_features=256, bias=True)\n",
      "        )\n",
      "        (1): SegformerMLP(\n",
      "          (proj): Linear(in_features=64, out_features=256, bias=True)\n",
      "        )\n",
      "        (2): SegformerMLP(\n",
      "          (proj): Linear(in_features=160, out_features=256, bias=True)\n",
      "        )\n",
      "        (3): SegformerMLP(\n",
      "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (linear_fuse): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (classifier): Conv2d(256, 180, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=5760, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n",
    "\n",
    "pretrained_model_name = \"nvidia/mit-b0\"\n",
    "nn_model = AutoModelForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self. nn_model = nn_model\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5760, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        #print(\"x   \", kwargs)\n",
    "        x = self. nn_model(pixel_values=kwargs['pixel_values'])\n",
    "        logits = x.logits\n",
    "        output = torch.sigmoid(logits)[0]\n",
    "        #output = np.transpose(output, (1,2,0))\n",
    "        output = output.view(2,-1)\n",
    "        #print(\"out   \",output)\n",
    "        #print(\"out   \",output.shape)\n",
    "        logits = self.linear_relu_stack(output)\n",
    "        #print(\"logits   \", logits.shape)\n",
    "        #print(\"logits   \", logits)\n",
    "        #logits = logits.argmax(axis=1)\n",
    "        logits = torch.softmax(logits,dim=-1)\n",
    "        #print(\"logits   \", logits.shape)\n",
    "        #print(\"logits   \", logits)\n",
    "        return torch.mean(logits), _\n",
    "    \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 21\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 55\n",
      "  Number of trainable parameters = 6977822\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to segformer-b0-scene-parse-150/checkpoint-20\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [segformer-b0-scene-parse-150/checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to segformer-b0-scene-parse-150/checkpoint-40\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [segformer-b0-scene-parse-150/checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 2\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=55, training_loss=0.10000000094825572, metrics={'train_runtime': 8.8475, 'train_samples_per_second': 11.868, 'train_steps_per_second': 6.216, 'total_flos': 0.0, 'train_loss': 0.10000000094825572, 'epoch': 5.0})"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"segformer-b0-scene-parse-150\",\n",
    "    learning_rate=6e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=5,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    #compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
